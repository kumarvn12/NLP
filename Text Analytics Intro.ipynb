{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "2453b6ca63716f3e98ce2f8b5d947f62506f417f"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import keras\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "8739accd2afecff56bc76d9d99a93062c8d909bf"
      },
      "cell_type": "markdown",
      "source": "Sentence Tokenization\n\nSentence tokenizer breaks text paragraph into sentences."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "60e4284d995a36cd211addc13bf837d7205d2c1a"
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import sent_tokenize\ntext=\"\"\"Hello Mr. Smith, how are you doing today? The weather is great, and city is awesome.\nThe sky is pinkish-blue. You shouldn't eat cardboard\"\"\"\ntokenized_text=sent_tokenize(text)\nprint(tokenized_text)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b48defb03ca5a017293fa9614f11b0321b16bf45"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "cfcdf7268f532e6d85e0e62f6fd6aec7e290c99a"
      },
      "cell_type": "markdown",
      "source": "Word tokenizer breaks text paragraph into words."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3aafc6d9ecdba0e233b187a2e8a6caca9323939e"
      },
      "cell_type": "code",
      "source": "from nltk.tokenize import word_tokenize\ntokenized_word=word_tokenize(text)\nprint(tokenized_word)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d131c30d99ac5538eb732bb653fff6244ef10c1f"
      },
      "cell_type": "code",
      "source": "#Frequency Distribution\nfrom nltk.probability import FreqDist\nfdist = FreqDist(tokenized_word)\nprint(fdist)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7e43481dd9307466abd407fed5782397e57559a9"
      },
      "cell_type": "code",
      "source": "fdist.most_common(2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ed0615ef2a3aebf068c02231b9721dcc61a51170"
      },
      "cell_type": "code",
      "source": "# Frequency Distribution Plot\nimport matplotlib.pyplot as plt\nfdist.plot(30,cumulative=False)\nplt.show()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f4f8c1a1041b0a5588d9546132196f1e49d7f428"
      },
      "cell_type": "code",
      "source": "#stopwords\nfrom nltk.corpus import stopwords\nstop_words=set(stopwords.words(\"english\"))\nprint(stop_words)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "dbcce3887c31602778cf2d700c0fc20cdcd62e08"
      },
      "cell_type": "code",
      "source": "Tokenized_Sent = ['Hello', 'Mr.', 'Smith', ',', 'how', 'are', 'you', 'doing', 'today', '?']",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "175c1bd0496178d1824979467a391029210091f2"
      },
      "cell_type": "code",
      "source": "filtered_Sent=[]\nfor w in Tokenized_Sent:\n    if w not in stop_words:\n        filtered_sent.append(w)\nprint(\"Tokenized Sentence:\",Tokenized_Sent)\nprint(\"Filterd Sentence:\",filtered_sent)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "af9a78713aa6b9a9f44a8c1f234efc61872bbad5"
      },
      "cell_type": "code",
      "source": "#Lexicon Normalization\n#performing stemming and Lemmatization\n\nfrom nltk.stem.wordnet import WordNetLemmatizer\nlem = WordNetLemmatizer()\n\nfrom nltk.stem.porter import PorterStemmer\nstem = PorterStemmer()\n\nword = \"flying\"\nprint(\"Lemmatized Word:\",lem.lemmatize(word,\"v\"))\nprint(\"Stemmed Word:\",stem.stem(word))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "f349864117b5be34f4c421f6d638b42b6e22558d"
      },
      "cell_type": "code",
      "source": "#POS Tagging\nsent = \"Albert Einstein was born in Ulm, Germany in 1879.\"",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "ad7b526d7e6a4c362c5970c05a123ec4f5f5e1c2"
      },
      "cell_type": "markdown",
      "source": "N-Grams as Features\n\nA combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text."
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "59e8002c37035075ff3da850accd3d6d808e09d7"
      },
      "cell_type": "code",
      "source": "text = \"A combination of N words together are called N-Grams. N grams (N > 1) are generally more informative as compared to words (Unigrams) as features.\"\n\ndef generate_ngrams(text, n):\n    words = text.split()\n    output = []  \n    for i in range(len(words)-n+1):\n        output.append(words[i:i+n])\n    return output\n\ngenerate_ngrams(text, 2)",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 4,
          "data": {
            "text/plain": "[['A', 'combination'],\n ['combination', 'of'],\n ['of', 'N'],\n ['N', 'words'],\n ['words', 'together'],\n ['together', 'are'],\n ['are', 'called'],\n ['called', 'N-Grams.'],\n ['N-Grams.', 'N'],\n ['N', 'grams'],\n ['grams', '(N'],\n ['(N', '>'],\n ['>', '1)'],\n ['1)', 'are'],\n ['are', 'generally'],\n ['generally', 'more'],\n ['more', 'informative'],\n ['informative', 'as'],\n ['as', 'compared'],\n ['compared', 'to'],\n ['to', 'words'],\n ['words', '(Unigrams)'],\n ['(Unigrams)', 'as'],\n ['as', 'features.']]"
          },
          "metadata": {}
        }
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "d5340f10c7d6786eeb7feacd45a013751ba0168b"
      },
      "cell_type": "code",
      "source": "import nltk",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "3924ecc2da9c383d7ba924484c3f4a67fd3fa3fa"
      },
      "cell_type": "code",
      "source": "tokens = nltk.word_tokenize(sent)\nprint(tokens)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "c3f75e8c9a38542b9e84bf21721cba41e93ee512"
      },
      "cell_type": "code",
      "source": "nltk.pos_tag(tokens)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_uuid": "65165d57c7e58dce74f5d9bc6c77de7855017ea6"
      },
      "cell_type": "markdown",
      "source": "CC - coordinating conjunction\nRB -  adverbs\nIN - preposition\nNN - noun\nJJ - an adjective."
    },
    {
      "metadata": {
        "trusted": true,
        "scrolled": true,
        "_uuid": "96bf7bf9352763c3c9b76edfd31f43b9e44e1021"
      },
      "cell_type": "code",
      "source": "#TF-IDF\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nobj = TfidfVectorizer()\ncorpus = ['This is sample document.', 'another random document.', 'third sample document text']\nX = obj.fit_transform(corpus)\nprint(X)",
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": "  (0, 7)\t0.5844829010200651\n  (0, 2)\t0.5844829010200651\n  (0, 4)\t0.444514311537431\n  (0, 1)\t0.34520501686496574\n  (1, 1)\t0.3853716274664007\n  (1, 0)\t0.652490884512534\n  (1, 3)\t0.652490884512534\n  (2, 4)\t0.444514311537431\n  (2, 1)\t0.34520501686496574\n  (2, 6)\t0.5844829010200651\n  (2, 5)\t0.5844829010200651\n",
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "_uuid": "b5a79b990e0ab13e9f56b04bd3676ad562b6f2ba"
      },
      "cell_type": "markdown",
      "source": "The model creates a vocabulary dictionary and assigns an index to each word. Each row in the output contains a tuple (i,j) and a tf-idf value of word at index j in document i."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}